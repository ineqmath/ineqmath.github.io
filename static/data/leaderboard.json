[
  {
    "Model": "DeepSeek-R1-0528",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250528",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-28",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-12 03:12:18.689983",
    "is_correct": {
      "total": {
        "correct": 24,
        "incorrect": 176,
        "accuracy": 12.0
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.3
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 68,
        "incorrect": 132,
        "accuracy": 34.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.2
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 65,
        "incorrect": 135,
        "accuracy": 32.5
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.3
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 58,
        "incorrect": 142,
        "accuracy": 29.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 32,
        "incorrect": 64,
        "accuracy": 33.3
      }
    },
    "calc_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.6
      }
    },
    "all_true": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "run_dir": "DeepSeek-R1-0528_tokens_10000"
  },
  {
    "Model": "DeepSeek-R1-0528",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250528",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-28",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-12 22:26:20.132860",
    "is_correct": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 71,
        "incorrect": 33,
        "accuracy": 68.3
      },
      "bound": {
        "correct": 75,
        "incorrect": 21,
        "accuracy": 78.1
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.0
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 49.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 104,
        "incorrect": 96,
        "accuracy": 52.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.6
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.3
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.6
      }
    },
    "all_true": {
      "total": {
        "correct": 19,
        "incorrect": 181,
        "accuracy": 9.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.5
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "run_dir": "DeepSeek-R1-0528_tokens_40000"
  },
  {
    "Model": "DeepSeek-R1",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "Date": "2025-01-19",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 65,
        "incorrect": 31,
        "accuracy": 67.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 114,
        "incorrect": 86,
        "accuracy": 57.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.6
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.9
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 70,
        "incorrect": 34,
        "accuracy": 67.3
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "run_dir": "DeepSeek-R1_tokens_10000"
  },
  {
    "Model": "DeepSeek-V3-0324",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://api-docs.deepseek.com/news/news250325",
    "Reasoning effort": "not applicable",
    "Date": "2025-03-25",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-12 03:09:08.485402",
    "is_correct": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.2
      },
      "bound": {
        "correct": 63,
        "incorrect": 33,
        "accuracy": 65.6
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.2
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 30,
        "incorrect": 170,
        "accuracy": 15.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.4
      },
      "bound": {
        "correct": 15,
        "incorrect": 81,
        "accuracy": 15.6
      }
    },
    "approx_judge": {
      "total": {
        "correct": 126,
        "incorrect": 74,
        "accuracy": 63.0
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.5
      },
      "bound": {
        "correct": 60,
        "incorrect": 36,
        "accuracy": 62.5
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 14,
        "incorrect": 186,
        "accuracy": 7.0
      },
      "relation": {
        "correct": 8,
        "incorrect": 96,
        "accuracy": 7.7
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "run_dir": "DeepSeek-V3-0324_tokens_10000"
  },
  {
    "Model": "DeepSeek-V3.1 (Thinking Mode)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-21",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-24 03:14:42.936489",
    "is_correct": {
      "total": {
        "correct": 65,
        "incorrect": 135,
        "accuracy": 32.5
      },
      "relation": {
        "correct": 49,
        "incorrect": 55,
        "accuracy": 47.1
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 153,
        "incorrect": 47,
        "accuracy": 76.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.2
      },
      "bound": {
        "correct": 79,
        "incorrect": 17,
        "accuracy": 82.3
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 54,
        "incorrect": 146,
        "accuracy": 27.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 176,
        "incorrect": 24,
        "accuracy": 88.0
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.9
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 24,
        "incorrect": 176,
        "accuracy": 12.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.4
      },
      "bound": {
        "correct": 9,
        "incorrect": 87,
        "accuracy": 9.4
      }
    },
    "run_dir": "DeepSeek-V3.1 (Thinking Mode)_tokens_10000"
  },
  {
    "Model": "DeepSeek-V3.1 (Thinking Mode)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "671B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-21",
    "Max Tokens": 30000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-24 02:44:05.611544",
    "is_correct": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 73,
        "incorrect": 31,
        "accuracy": 70.2
      },
      "bound": {
        "correct": 73,
        "incorrect": 23,
        "accuracy": 76.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 104,
        "incorrect": 96,
        "accuracy": 52.0
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 46,
        "incorrect": 50,
        "accuracy": 47.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.9
      }
    },
    "approx_judge": {
      "total": {
        "correct": 129,
        "incorrect": 71,
        "accuracy": 64.5
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.7
      },
      "bound": {
        "correct": 69,
        "incorrect": 27,
        "accuracy": 71.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.3
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "run_dir": "DeepSeek-V3.1 (Thinking Mode)_tokens_30000"
  },
  {
    "Model": "Kimi K2 Instruct",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "1000B",
    "Source": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
    "Reasoning effort": "not applicable",
    "Date": "2025-07-12",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-07-20 20:07:09.372066",
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 52,
        "incorrect": 52,
        "accuracy": 50.0
      },
      "bound": {
        "correct": 73,
        "incorrect": 23,
        "accuracy": 76.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 67,
        "incorrect": 133,
        "accuracy": 33.5
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 51,
        "incorrect": 149,
        "accuracy": 25.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 105,
        "incorrect": 95,
        "accuracy": 52.5
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.2
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.4
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "all_true": {
      "total": {
        "correct": 18,
        "incorrect": 182,
        "accuracy": 9.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.7
      },
      "bound": {
        "correct": 9,
        "incorrect": 87,
        "accuracy": 9.4
      }
    },
    "run_dir": "Kimi-K2-Instruct_tokens_10000"
  },
  {
    "Model": "Qwen3-4B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "4B",
    "Source": "https://huggingface.co/Qwen/Qwen3-4B",
    "Reasoning effort": "not applicable",
    "Date": "2025-04-27",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-07-20 21:02:39.689619",
    "is_correct": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.5
      },
      "bound": {
        "correct": 48,
        "incorrect": 48,
        "accuracy": 50.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.2
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 64,
        "incorrect": 136,
        "accuracy": 32.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 171,
        "incorrect": 29,
        "accuracy": 85.5
      },
      "relation": {
        "correct": 89,
        "incorrect": 15,
        "accuracy": 85.6
      },
      "bound": {
        "correct": 82,
        "incorrect": 14,
        "accuracy": 85.4
      }
    },
    "all_true": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.2
      }
    },
    "run_dir": "Qwen3-4B-2025-04-27_tokens_10000_20250720210239"
  },
  {
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.9
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.6
      },
      "bound": {
        "correct": 35,
        "incorrect": 61,
        "accuracy": 36.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.6
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "claude-3-7-sonnet-20250219_tokens_10000"
  },
  {
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "8000",
    "is_correct": {
      "total": {
        "correct": 83,
        "incorrect": 117,
        "accuracy": 41.5
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.7
      },
      "bound": {
        "correct": 48,
        "incorrect": 48,
        "accuracy": 50.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.5
      },
      "bound": {
        "correct": 32,
        "incorrect": 64,
        "accuracy": 33.3
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.4
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "all_true": {
      "total": {
        "correct": 2,
        "incorrect": 198,
        "accuracy": 1.0
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 1.0
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0
      }
    },
    "run_dir": "claude-3-7-sonnet-20250219_tokens_8000"
  },
  {
    "Model": "Claude Opus 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://www.anthropic.com/news/claude-4",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-14",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-12 00:45:16.535946",
    "is_correct": {
      "total": {
        "correct": 95,
        "incorrect": 105,
        "accuracy": 47.5
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.3
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 50,
        "incorrect": 150,
        "accuracy": 25.0
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.0
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 18,
        "incorrect": 182,
        "accuracy": 9.0
      },
      "relation": {
        "correct": 11,
        "incorrect": 93,
        "accuracy": 10.6
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 179,
        "incorrect": 21,
        "accuracy": 89.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "all_true": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.2
      }
    },
    "run_dir": "claude-oups-4-20250514_tokens_10000"
  },
  {
    "Model": "Claude Sonnet 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://www.anthropic.com/news/claude-4",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-14",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-12 00:49:56.782607",
    "is_correct": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 33,
        "incorrect": 71,
        "accuracy": 31.7
      },
      "bound": {
        "correct": 55,
        "incorrect": 41,
        "accuracy": 57.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 39,
        "incorrect": 161,
        "accuracy": 19.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 13,
        "incorrect": 187,
        "accuracy": 6.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.1
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "run_dir": "claude-sonnet-4-20250514_tokens_10000"
  },
  {
    "Model": "Gemini 2.0 Flash-Lite",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://deepmind.google/technologies/gemini/flash-lite/",
    "Date": "2025-02-25",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 66,
        "incorrect": 134,
        "accuracy": 33.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.1
      },
      "bound": {
        "correct": 42,
        "incorrect": 54,
        "accuracy": 43.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 23,
        "incorrect": 177,
        "accuracy": 11.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.5
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 72,
        "incorrect": 32,
        "accuracy": 69.2
      },
      "bound": {
        "correct": 74,
        "incorrect": 22,
        "accuracy": 77.1
      }
    },
    "calc_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 1.0
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "gemini-2.0-flash-lite_tokens_10000"
  },
  {
    "Model": "Gemini 2.0 Flash",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash",
    "Date": "2025-02-05",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 57,
        "incorrect": 39,
        "accuracy": 59.4
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.3
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 111,
        "incorrect": 89,
        "accuracy": 55.5
      },
      "relation": {
        "correct": 53,
        "incorrect": 51,
        "accuracy": 51.0
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.4
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "run_dir": "gemini-2.0-flash_tokens_10000"
  },
  {
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.7
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 176,
        "incorrect": 24,
        "accuracy": 88.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.4
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "run_dir": "gemini-2.5-flash-preview-04-17_tokens_10000"
  },
  {
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "40000",
    "is_correct": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.3
      },
      "bound": {
        "correct": 46,
        "incorrect": 50,
        "accuracy": 47.9
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.5
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 73,
        "incorrect": 127,
        "accuracy": 36.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.2
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 47,
        "incorrect": 153,
        "accuracy": 23.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.0
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.9
      }
    },
    "run_dir": "gemini-2.5-flash-preview-04-17_tokens_40000"
  },
  {
    "Model": "Gemini 2.5 Flash Preview 05-20",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-05-20",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-20",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-14 02:13:04.549577",
    "is_correct": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.2
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 164,
        "incorrect": 36,
        "accuracy": 82.0
      },
      "relation": {
        "correct": 87,
        "incorrect": 17,
        "accuracy": 83.7
      },
      "bound": {
        "correct": 77,
        "incorrect": 19,
        "accuracy": 80.2
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 38,
        "incorrect": 66,
        "accuracy": 36.5
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.5
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.3
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "run_dir": "gemini-2.5-flash-preview-05-20_tokens_10000"
  },
  {
    "Model": "Gemini 2.5 Flash Preview 05-20",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-flash-preview-05-20",
    "Reasoning effort": "not applicable",
    "Date": "2025-05-20",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-13 23:52:28.624851",
    "is_correct": {
      "total": {
        "correct": 90,
        "incorrect": 110,
        "accuracy": 45.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 166,
        "incorrect": 34,
        "accuracy": 83.0
      },
      "relation": {
        "correct": 89,
        "incorrect": 15,
        "accuracy": 85.6
      },
      "bound": {
        "correct": 77,
        "incorrect": 19,
        "accuracy": 80.2
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 88,
        "incorrect": 112,
        "accuracy": 44.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.2
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 178,
        "incorrect": 22,
        "accuracy": 89.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.5
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 192,
        "incorrect": 8,
        "accuracy": 96.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "all_true": {
      "total": {
        "correct": 55,
        "incorrect": 145,
        "accuracy": 27.5
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.9
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "run_dir": "gemini-2.5-flash-preview-05-20_tokens_40000"
  },
  {
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 14,
        "incorrect": 186,
        "accuracy": 7.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 80,
        "incorrect": 16,
        "accuracy": 83.3
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.8
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "run_dir": "gemini-2.5-pro-preview-03-25_tokens_10000"
  },
  {
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "30000",
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 64,
        "incorrect": 40,
        "accuracy": 61.5
      },
      "bound": {
        "correct": 72,
        "incorrect": 24,
        "accuracy": 75.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.5
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 126,
        "incorrect": 74,
        "accuracy": 63.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.4
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.4
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.5
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "all_true": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.5
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 49.0
      }
    },
    "run_dir": "gemini-2.5-pro-preview-03-25_tokens_30000"
  },
  {
    "Model": "Gemini 2.5 Pro Preview",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-06-05",
    "Reasoning effort": "not applicable",
    "Date": "2025-06-05",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-14 00:59:39.880539",
    "is_correct": {
      "total": {
        "correct": 26,
        "incorrect": 174,
        "accuracy": 13.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.5
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.2
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 20,
        "incorrect": 180,
        "accuracy": 10.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 14,
        "incorrect": 82,
        "accuracy": 14.6
      }
    },
    "run_dir": "gemini-2.5-pro-preview-06-05_tokens_10000"
  },
  {
    "Model": "Gemini 2.5 Pro Preview",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.5-pro-preview-06-05",
    "Reasoning effort": "not applicable",
    "Date": "2025-06-05",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-06-14 00:57:36.831162",
    "is_correct": {
      "total": {
        "correct": 132,
        "incorrect": 68,
        "accuracy": 66.0
      },
      "relation": {
        "correct": 57,
        "incorrect": 47,
        "accuracy": 54.8
      },
      "bound": {
        "correct": 75,
        "incorrect": 21,
        "accuracy": 78.1
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 170,
        "incorrect": 30,
        "accuracy": 85.0
      },
      "relation": {
        "correct": 85,
        "incorrect": 19,
        "accuracy": 81.7
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 130,
        "incorrect": 70,
        "accuracy": 65.0
      },
      "relation": {
        "correct": 64,
        "incorrect": 40,
        "accuracy": 61.5
      },
      "bound": {
        "correct": 66,
        "incorrect": 30,
        "accuracy": 68.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.4
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 92,
        "incorrect": 108,
        "accuracy": 46.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 53,
        "incorrect": 43,
        "accuracy": 55.2
      }
    },
    "run_dir": "gemini-2.5-pro-preview-06-05_tokens_40000"
  },
  {
    "Model": "GPT-4.1",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/gpt-4.1",
    "Date": "2025-04-14",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.0
      },
      "bound": {
        "correct": 30,
        "incorrect": 66,
        "accuracy": 31.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 20,
        "incorrect": 180,
        "accuracy": 10.0
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.5
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 119,
        "incorrect": 81,
        "accuracy": 59.5
      },
      "relation": {
        "correct": 55,
        "incorrect": 49,
        "accuracy": 52.9
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.8
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "gpt-4.1-2025-04-14_tokens_10000"
  },
  {
    "Model": "GPT-4o mini",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/gpt-4o-mini",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 38,
        "incorrect": 66,
        "accuracy": 36.5
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 58,
        "incorrect": 142,
        "accuracy": 29.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.2
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 180,
        "incorrect": 20,
        "accuracy": 90.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.5
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0
      }
    },
    "run_dir": "gpt-4o-mini_tokens_10000"
  },
  {
    "Model": "GPT-4o",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/gpt-4o",
    "Date": "2024-08-06",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 75,
        "incorrect": 125,
        "accuracy": 37.5
      },
      "relation": {
        "correct": 36,
        "incorrect": 68,
        "accuracy": 34.6
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.6
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 64,
        "incorrect": 136,
        "accuracy": 32.0
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.3
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "gpt-4o_tokens_10000"
  },
  {
    "Model": "GPT-5",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-08 07:16:02.560565",
    "is_correct": {
      "total": {
        "correct": 73,
        "incorrect": 127,
        "accuracy": 36.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.5
      },
      "bound": {
        "correct": 33,
        "incorrect": 63,
        "accuracy": 34.4
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 94,
        "incorrect": 106,
        "accuracy": 47.0
      },
      "relation": {
        "correct": 50,
        "incorrect": 54,
        "accuracy": 48.1
      },
      "bound": {
        "correct": 44,
        "incorrect": 52,
        "accuracy": 45.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.4
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 194,
        "incorrect": 6,
        "accuracy": 97.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 56,
        "incorrect": 144,
        "accuracy": 28.0
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.0
      },
      "bound": {
        "correct": 31,
        "incorrect": 65,
        "accuracy": 32.3
      }
    },
    "run_dir": "gpt-5-2025-08-07_tokens_10000"
  },
  {
    "Model": "GPT-5",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 30000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-08 07:59:45.464547",
    "is_correct": {
      "total": {
        "correct": 133,
        "incorrect": 67,
        "accuracy": 66.5
      },
      "relation": {
        "correct": 71,
        "incorrect": 33,
        "accuracy": 68.3
      },
      "bound": {
        "correct": 62,
        "incorrect": 34,
        "accuracy": 64.6
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 138,
        "incorrect": 62,
        "accuracy": 69.0
      },
      "relation": {
        "correct": 72,
        "incorrect": 32,
        "accuracy": 69.2
      },
      "bound": {
        "correct": 66,
        "incorrect": 30,
        "accuracy": 68.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.4
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 94,
        "incorrect": 106,
        "accuracy": 47.0
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.3
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.1
      }
    },
    "run_dir": "gpt-5-2025-08-07_tokens_30000"
  },
  {
    "Model": "GPT-5 mini",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://platform.openai.com/docs/models/gpt-5-mini",
    "Reasoning effort": "medium",
    "Date": "2025-08-07",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-08 07:16:45.520846",
    "is_correct": {
      "total": {
        "correct": 111,
        "incorrect": 89,
        "accuracy": 55.5
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.7
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.1
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 100,
        "incorrect": 100,
        "accuracy": 50.0
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 42,
        "incorrect": 54,
        "accuracy": 43.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 61,
        "incorrect": 139,
        "accuracy": 30.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "run_dir": "gpt-5-mini-2025-08-07_tokens_10000"
  },
  {
    "Model": "gpt-oss-120b",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "120B",
    "Source": "https://huggingface.co/openai/gpt-oss-120b",
    "Reasoning effort": "not applicable",
    "Date": "2025-08-05",
    "Max Tokens": 10000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-08-06 06:44:21.573012",
    "is_correct": {
      "total": {
        "correct": 133,
        "incorrect": 67,
        "accuracy": 66.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.2
      },
      "bound": {
        "correct": 59,
        "incorrect": 37,
        "accuracy": 61.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 80,
        "incorrect": 120,
        "accuracy": 40.0
      },
      "relation": {
        "correct": 42,
        "incorrect": 62,
        "accuracy": 40.4
      },
      "bound": {
        "correct": 38,
        "incorrect": 58,
        "accuracy": 39.6
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.1
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.6
      }
    },
    "all_true": {
      "total": {
        "correct": 47,
        "incorrect": 153,
        "accuracy": 23.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.0
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.9
      }
    },
    "run_dir": "gpt-oss-120b-2025-08-05_tokens_10000"
  },
  {
    "Model": "Grok 3",
    "Type": "Chat",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://x.ai/news/grok-3",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.5
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.3
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 34,
        "incorrect": 166,
        "accuracy": 17.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.2
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.2
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.8
      },
      "bound": {
        "correct": 40,
        "incorrect": 56,
        "accuracy": 41.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.4
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "run_dir": "grok-3-latest_tokens_10000"
  },
  {
    "Model": "Grok 3 mini",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://x.ai/news/grok-3",
    "Reasoning effort": "medium",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 143,
        "incorrect": 57,
        "accuracy": 71.5
      },
      "relation": {
        "correct": 73,
        "incorrect": 31,
        "accuracy": 70.2
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.9
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 48,
        "incorrect": 152,
        "accuracy": 24.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.8
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.7
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 39,
        "incorrect": 161,
        "accuracy": 19.5
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.9
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.2
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.5
      }
    },
    "calc_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 8,
        "incorrect": 96,
        "accuracy": 7.7
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "run_dir": "grok-3-mini-latest_tokens_10000"
  },
  {
    "Model": "Grok 4",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "UNK",
    "Source": "https://docs.x.ai/docs/models/grok-4-0709",
    "Reasoning effort": "not applicable",
    "Date": "2025-07-09",
    "Max Tokens": 40000,
    "Email": "ineqmath_admin@berkeley.edu",
    "timestamp": "2025-07-12 21:14:12.977858",
    "is_correct": {
      "total": {
        "correct": 152,
        "incorrect": 48,
        "accuracy": 76.0
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.2
      },
      "bound": {
        "correct": 78,
        "incorrect": 18,
        "accuracy": 81.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 114,
        "incorrect": 86,
        "accuracy": 57.0
      },
      "relation": {
        "correct": 75,
        "incorrect": 29,
        "accuracy": 72.1
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.6
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 28,
        "incorrect": 172,
        "accuracy": 14.0
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.3
      }
    },
    "approx_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 16,
        "incorrect": 184,
        "accuracy": 8.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.7
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "run_dir": "grok-4-0709_tokens_40000"
  },
  {
    "Model": "o1",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Reasoning effort": "medium",
    "Date": "2024-12-17",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 65,
        "incorrect": 39,
        "accuracy": 62.5
      },
      "bound": {
        "correct": 60,
        "incorrect": 36,
        "accuracy": 62.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 69,
        "incorrect": 131,
        "accuracy": 34.5
      },
      "relation": {
        "correct": 33,
        "incorrect": 71,
        "accuracy": 31.7
      },
      "bound": {
        "correct": 36,
        "incorrect": 60,
        "accuracy": 37.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.1
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 78,
        "incorrect": 26,
        "accuracy": 75.0
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 16,
        "incorrect": 184,
        "accuracy": 8.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.7
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "run_dir": "o1_tokens_10000"
  },
  {
    "Model": "o1",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Reasoning effort": "medium",
    "Date": "2024-12-17",
    "Max Tokens": "40000",
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.3
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 57,
        "incorrect": 143,
        "accuracy": 28.5
      },
      "relation": {
        "correct": 30,
        "incorrect": 74,
        "accuracy": 28.8
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.2
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 77,
        "incorrect": 27,
        "accuracy": 74.0
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.7
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "run_dir": "o1_tokens_40000"
  },
  {
    "Model": "o3-mini",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o3-mini",
    "Reasoning effort": "medium",
    "Date": "2025-01-31",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 33,
        "incorrect": 63,
        "accuracy": 34.4
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 44,
        "incorrect": 156,
        "accuracy": 22.0
      },
      "relation": {
        "correct": 27,
        "incorrect": 77,
        "accuracy": 26.0
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.7
      }
    },
    "approx_judge": {
      "total": {
        "correct": 155,
        "incorrect": 45,
        "accuracy": 77.5
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.5
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 19,
        "incorrect": 181,
        "accuracy": 9.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.5
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "run_dir": "o3-mini-2025-01-31_tokens_10000"
  },
  {
    "Model": "o3-pro",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o3-pro",
    "Reasoning effort": "medium",
    "Date": "2025-06-10",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.2
      },
      "bound": {
        "correct": 62,
        "incorrect": 34,
        "accuracy": 64.6
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 134,
        "incorrect": 66,
        "accuracy": 67.0
      },
      "relation": {
        "correct": 76,
        "incorrect": 28,
        "accuracy": 73.1
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.4
      }
    },
    "approx_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.9
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 91,
        "incorrect": 109,
        "accuracy": 45.5
      },
      "relation": {
        "correct": 48,
        "incorrect": 56,
        "accuracy": 46.2
      },
      "bound": {
        "correct": 43,
        "incorrect": 53,
        "accuracy": 44.8
      }
    },
    "run_dir": "o3-pro_tokens_10000"
  },
  {
    "Model": "o3-pro",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o3-pro",
    "Reasoning effort": "medium",
    "Date": "2025-06-10",
    "Max Tokens": "40000",
    "is_correct": {
      "total": {
        "correct": 137,
        "incorrect": 62,
        "accuracy": 68.5
      },
      "relation": {
        "correct": 74,
        "incorrect": 30,
        "accuracy": 71.2
      },
      "bound": {
        "correct": 63,
        "incorrect": 32,
        "accuracy": 66.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 190,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 147,
        "incorrect": 52,
        "accuracy": 73.5
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.8
      },
      "bound": {
        "correct": 65,
        "incorrect": 30,
        "accuracy": 68.4
      }
    },
    "approx_judge": {
      "total": {
        "correct": 172,
        "incorrect": 27,
        "accuracy": 86.0
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.8
      },
      "bound": {
        "correct": 90,
        "incorrect": 5,
        "accuracy": 94.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 10,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "all_true": {
      "total": {
        "correct": 92,
        "incorrect": 107,
        "accuracy": 46.0
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.3
      },
      "bound": {
        "correct": 47,
        "incorrect": 48,
        "accuracy": 49.5
      }
    },
    "run_dir": "o3-pro_tokens_40000"
  },
  {
    "Model": "o3",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Reasoning effort": "medium",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.3
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 52,
        "incorrect": 52,
        "accuracy": 50.0
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 183,
        "incorrect": 17,
        "accuracy": 91.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 194,
        "incorrect": 6,
        "accuracy": 97.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.1
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "all_true": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.1
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.8
      }
    },
    "run_dir": "o3_tokens_10000"
  },
  {
    "Model": "o3",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Reasoning effort": "medium",
    "Date": "2025-04-16",
    "Max Tokens": "40000",
    "is_correct": {
      "total": {
        "correct": 144,
        "incorrect": 56,
        "accuracy": 72.0
      },
      "relation": {
        "correct": 76,
        "incorrect": 28,
        "accuracy": 73.1
      },
      "bound": {
        "correct": 68,
        "incorrect": 28,
        "accuracy": 70.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 112,
        "incorrect": 88,
        "accuracy": 56.0
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.7
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.9
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.8
      }
    },
    "all_true": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.7
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.6
      }
    },
    "run_dir": "o3_tokens_40000"
  },
  {
    "Model": "o4-mini",
    "Type": "Reasoning",
    "Model Source": "Proprietary",
    "Size": "",
    "Source": "https://platform.openai.com/docs/models/o4-mini",
    "Reasoning effort": "medium",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 130,
        "incorrect": 70,
        "accuracy": 65.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.3
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 124,
        "incorrect": 76,
        "accuracy": 62.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.4
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.3
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.9
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 86,
        "incorrect": 18,
        "accuracy": 82.7
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.6
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.3
      },
      "bound": {
        "correct": 14,
        "incorrect": 82,
        "accuracy": 14.6
      }
    },
    "run_dir": "o4-mini-2025-04-16_tokens_10000"
  },
  {
    "Model": "DeepSeek-R1 (Llama-70B)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "70B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.0
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 46,
        "incorrect": 154,
        "accuracy": 23.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.1
      },
      "bound": {
        "correct": 23,
        "incorrect": 73,
        "accuracy": 24.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.8
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.5
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.6
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.2
      }
    },
    "run_dir": "together-DeepSeek-R1-Distill-Llama-70B_tokens_10000"
  },
  {
    "Model": "DeepSeek-R1 (Qwen-1.5B)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "1.5B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.5
      },
      "relation": {
        "correct": 13,
        "incorrect": 91,
        "accuracy": 12.5
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.9
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.7
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 96,
        "incorrect": 104,
        "accuracy": 48.0
      },
      "relation": {
        "correct": 49,
        "incorrect": 55,
        "accuracy": 47.1
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 49.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 1.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "together-DeepSeek-R1-Distill-Qwen-1.5B_tokens_10000"
  },
  {
    "Model": "DeepSeek-R1 (Qwen-14B)",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "14B",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.5
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.7
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.1
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.8
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.1
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.5
      }
    },
    "calc_judge": {
      "total": {
        "correct": 170,
        "incorrect": 30,
        "accuracy": 85.0
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.8
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "run_dir": "together-DeepSeek-R1-Distill-Qwen-14B_tokens_10000"
  },
  {
    "Model": "Llama-3.2-3B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "3B",
    "Source": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "Date": "2024-09-25",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 22,
        "incorrect": 178,
        "accuracy": 11.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.4
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.3
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 164,
        "incorrect": 36,
        "accuracy": 82.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 69,
        "incorrect": 27,
        "accuracy": 71.9
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.1
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "together-Llama-3.2-3B-Instruct-Turbo_tokens_10000"
  },
  {
    "Model": "Llama-4-Maverick",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "128 x 17B",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 37,
        "incorrect": 67,
        "accuracy": 35.6
      },
      "bound": {
        "correct": 44,
        "incorrect": 52,
        "accuracy": 45.8
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 85,
        "incorrect": 115,
        "accuracy": 42.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 178,
        "incorrect": 22,
        "accuracy": 89.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.5
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.7
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.1
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "together-Llama-4-Maverick-17B-128E-Instruct-FP8_tokens_10000"
  },
  {
    "Model": "Llama-4-Scout",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "16 x 17B",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 67,
        "incorrect": 133,
        "accuracy": 33.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.2
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.9
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 61,
        "incorrect": 139,
        "accuracy": 30.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.2
      },
      "bound": {
        "correct": 15,
        "incorrect": 81,
        "accuracy": 15.6
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.9
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 1.0
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "together-Llama-4-Scout-17B-16E-Instruct_tokens_10000"
  },
  {
    "Model": "Llama-3.1-8B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "8B",
    "Source": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.5
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.1
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.2
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.4
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.5
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "together-Meta-Llama-3.1-8B-Instruct-Turbo_tokens_10000"
  },
  {
    "Model": "QwQ-32B-preview",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "32B",
    "Source": "https://huggingface.co/Qwen/QwQ-32B-Preview",
    "Date": "2024-11-27",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 42,
        "incorrect": 62,
        "accuracy": 40.4
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.9
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 56,
        "incorrect": 144,
        "accuracy": 28.0
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.8
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 60,
        "incorrect": 140,
        "accuracy": 30.0
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.7
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 45,
        "incorrect": 155,
        "accuracy": 22.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.2
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.4
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.4
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "together-QwQ-32B-Preview_tokens_10000"
  },
  {
    "Model": "QwQ-32B",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "32B",
    "Source": "https://huggingface.co/Qwen/QwQ-32B",
    "Date": "2025-03-05",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.2
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.9
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 59,
        "incorrect": 141,
        "accuracy": 29.5
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.8
      }
    },
    "approx_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.2
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 79,
        "incorrect": 17,
        "accuracy": 82.3
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "together-QwQ-32B_tokens_10000"
  },
  {
    "Model": "Qwen2.5-72B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "72B",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.7
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.2
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.8
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "run_dir": "together-Qwen2.5-72B-Instruct-Turbo_tokens_10000"
  },
  {
    "Model": "Qwen2.5-7B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "7B",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.8
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.6
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.8
      },
      "bound": {
        "correct": 31,
        "incorrect": 65,
        "accuracy": 32.3
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.8
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.5
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.3
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.7
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "run_dir": "together-Qwen2.5-7B-Instruct-Turbo_tokens_10000"
  },
  {
    "Model": "Qwen2.5-Coder-32B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "32B",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
    "Date": "2024-11-10",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.8
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.2
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.1
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.1
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.6
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.6
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0
      }
    },
    "run_dir": "together-Qwen2.5-Coder-32B-Instruct_tokens_10000"
  },
  {
    "Model": "Qwen3-235B-A22B",
    "Type": "Reasoning",
    "Model Source": "Open-source",
    "Size": "235B",
    "Source": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
    "Date": "2025-04-28",
    "Max Tokens": "10000",
    "is_correct": {
      "total": {
        "correct": 82,
        "incorrect": 118,
        "accuracy": 41.0
      },
      "relation": {
        "correct": 48,
        "incorrect": 56,
        "accuracy": 46.2
      },
      "bound": {
        "correct": 34,
        "incorrect": 62,
        "accuracy": 35.4
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.4
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.2
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.2
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.7
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.1
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.3
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.8
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.7
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.1
      }
    },
    "run_dir": "together-Qwen3-235B-A22B-fp8-tput_tokens_10000"
  },
  {
    "Model": "Gemma-2-9B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "9B",
    "Source": "https://huggingface.co/google/gemma-2-9b-it",
    "Date": "2024-06-25",
    "Max Tokens": "6000",
    "is_correct": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.0
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3
      },
      "bound": {
        "correct": 71,
        "incorrect": 25,
        "accuracy": 74.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 99.0
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "together-gemma-2-9b-it_tokens_6000"
  },
  {
    "Model": "Gemma-2B",
    "Type": "Chat",
    "Model Source": "Open-source",
    "Size": "2B",
    "Source": "https://huggingface.co/google/gemma-2b-it",
    "Date": "2024-02-21",
    "Max Tokens": "6000",
    "is_correct": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 11,
        "incorrect": 93,
        "accuracy": 10.6
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.2
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 147,
        "incorrect": 53,
        "accuracy": 73.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.7
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.9
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.0
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.6
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "run_dir": "together-gemma-2b-it_tokens_6000"
  }
]