[
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "210_result.json",
      "192_result.json",
      "253_result.json"
    ],
    "Model": "Qwen2.5-Coder-32B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
    "Date": "2024-11-10",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 65,
        "incorrect": 39,
        "accuracy": 62.5
      },
      "bound": {
        "correct": 60,
        "incorrect": 36,
        "accuracy": 62.5
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 69,
        "incorrect": 131,
        "accuracy": 34.5
      },
      "relation": {
        "correct": 33,
        "incorrect": 71,
        "accuracy": 31.73076923076923
      },
      "bound": {
        "correct": 36,
        "incorrect": 60,
        "accuracy": 37.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 78,
        "incorrect": 26,
        "accuracy": 75.0
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 16,
        "incorrect": 184,
        "accuracy": 8.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "291_result.json",
      "281_result.json",
      "179_result.json",
      "228_result.json",
      "192_result.json",
      "266_result.json",
      "106_result.json",
      "289_result.json",
      "101_result.json",
      "253_result.json",
      "199_result.json",
      "293_result.json",
      "162_result.json",
      "120_result.json",
      "149_result.json",
      "235_result.json"
    ],
    "Model": "o1",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Date": "2024-12-17",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 144,
        "incorrect": 56,
        "accuracy": 72.0
      },
      "relation": {
        "correct": 76,
        "incorrect": 28,
        "accuracy": 73.07692307692307
      },
      "bound": {
        "correct": 68,
        "incorrect": 28,
        "accuracy": 70.83333333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 193,
        "incorrect": 7,
        "accuracy": 96.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 112,
        "incorrect": 88,
        "accuracy": 56.00000000000001
      },
      "relation": {
        "correct": 60,
        "incorrect": 44,
        "accuracy": 57.692307692307686
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.166666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 81,
        "incorrect": 23,
        "accuracy": 77.88461538461539
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "all_true_indices": [
      "147_result.json",
      "291_result.json",
      "232_result.json",
      "119_result.json",
      "175_result.json",
      "188_result.json",
      "298_result.json",
      "215_result.json",
      "181_result.json",
      "257_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "127_result.json",
      "179_result.json",
      "184_result.json",
      "252_result.json",
      "109_result.json",
      "142_result.json",
      "132_result.json",
      "205_result.json",
      "100_result.json",
      "103_result.json",
      "173_result.json",
      "258_result.json",
      "206_result.json",
      "297_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "166_result.json",
      "248_result.json",
      "266_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "134_result.json",
      "289_result.json",
      "171_result.json",
      "101_result.json",
      "133_result.json",
      "218_result.json",
      "268_result.json",
      "280_result.json",
      "156_result.json",
      "108_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "256_result.json",
      "180_result.json",
      "278_result.json",
      "111_result.json",
      "104_result.json",
      "189_result.json",
      "290_result.json",
      "168_result.json",
      "243_result.json",
      "293_result.json",
      "135_result.json",
      "159_result.json",
      "107_result.json",
      "162_result.json",
      "120_result.json",
      "225_result.json",
      "149_result.json",
      "212_result.json",
      "186_result.json",
      "130_result.json",
      "193_result.json",
      "229_result.json"
    ],
    "Model": "o3",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Date": "2025-04-16",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.57692307692307
      },
      "bound": {
        "correct": 35,
        "incorrect": 61,
        "accuracy": 36.45833333333333
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "106_result.json",
      "253_result.json",
      "278_result.json",
      "130_result.json"
    ],
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.50000000000001
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 34,
        "incorrect": 166,
        "accuracy": 17.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 40,
        "incorrect": 56,
        "accuracy": 41.66666666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.38461538461539
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "291_result.json",
      "103_result.json",
      "258_result.json",
      "106_result.json",
      "253_result.json",
      "189_result.json",
      "149_result.json"
    ],
    "Model": "Grok 3",
    "Type": "Chat",
    "Source": "https://x.ai/news/grok-3",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 11,
        "incorrect": 93,
        "accuracy": 10.576923076923077
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 147,
        "incorrect": 53,
        "accuracy": 73.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8076923076923
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Gemma-2B",
    "Type": "Chat",
    "Source": "https://huggingface.co/google/gemma-2b-it",
    "Date": "2024-02-21",
    "Max Tokens": "6000",
    "Model Source": "Open-source",
    "Size": "2B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 46,
        "incorrect": 50,
        "accuracy": 47.91666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.91666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 73,
        "incorrect": 127,
        "accuracy": 36.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 195,
        "incorrect": 5,
        "accuracy": 97.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 47,
        "incorrect": 153,
        "accuracy": 23.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 22,
        "incorrect": 74,
        "accuracy": 22.916666666666664
      }
    },
    "all_true_indices": [
      "232_result.json",
      "119_result.json",
      "175_result.json",
      "152_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "179_result.json",
      "237_result.json",
      "269_result.json",
      "205_result.json",
      "288_result.json",
      "275_result.json",
      "170_result.json",
      "103_result.json",
      "258_result.json",
      "206_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "166_result.json",
      "121_result.json",
      "266_result.json",
      "158_result.json",
      "128_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "289_result.json",
      "101_result.json",
      "218_result.json",
      "156_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "243_result.json",
      "145_result.json",
      "293_result.json",
      "135_result.json",
      "107_result.json",
      "120_result.json",
      "149_result.json",
      "212_result.json",
      "130_result.json",
      "229_result.json"
    ],
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 130,
        "incorrect": 70,
        "accuracy": 65.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.34615384615384
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.541666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 124,
        "incorrect": 76,
        "accuracy": 62.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.38461538461539
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.333333333333336
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 173,
        "incorrect": 27,
        "accuracy": 86.5
      },
      "relation": {
        "correct": 86,
        "incorrect": 18,
        "accuracy": 82.6923076923077
      },
      "bound": {
        "correct": 87,
        "incorrect": 9,
        "accuracy": 90.625
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 17,
        "incorrect": 87,
        "accuracy": 16.346153846153847
      },
      "bound": {
        "correct": 14,
        "incorrect": 82,
        "accuracy": 14.583333333333334
      }
    },
    "all_true_indices": [
      "291_result.json",
      "181_result.json",
      "198_result.json",
      "210_result.json",
      "281_result.json",
      "179_result.json",
      "184_result.json",
      "252_result.json",
      "275_result.json",
      "192_result.json",
      "282_result.json",
      "166_result.json",
      "151_result.json",
      "224_result.json",
      "176_result.json",
      "106_result.json",
      "289_result.json",
      "101_result.json",
      "190_result.json",
      "280_result.json",
      "253_result.json",
      "199_result.json",
      "261_result.json",
      "123_result.json",
      "189_result.json",
      "135_result.json",
      "239_result.json",
      "162_result.json",
      "296_result.json",
      "235_result.json",
      "229_result.json"
    ],
    "Model": "o4-mini",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o4-mini",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 143,
        "incorrect": 57,
        "accuracy": 71.5
      },
      "relation": {
        "correct": 73,
        "incorrect": 31,
        "accuracy": 70.1923076923077
      },
      "bound": {
        "correct": 70,
        "incorrect": 26,
        "accuracy": 72.91666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 48,
        "incorrect": 152,
        "accuracy": 24.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 39,
        "incorrect": 161,
        "accuracy": 19.5
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 61,
        "incorrect": 35,
        "accuracy": 63.541666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 8,
        "incorrect": 96,
        "accuracy": 7.6923076923076925
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "198_result.json",
      "191_result.json",
      "170_result.json",
      "103_result.json",
      "228_result.json",
      "248_result.json",
      "106_result.json",
      "289_result.json",
      "218_result.json",
      "253_result.json",
      "261_result.json",
      "293_result.json"
    ],
    "Model": "Grok 3 mini",
    "Type": "Reasoning",
    "Source": "https://x.ai/news/grok-3",
    "Date": "2025-02-19",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.75
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 19,
        "incorrect": 77,
        "accuracy": 19.791666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.54166666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 170,
        "incorrect": 30,
        "accuracy": 85.0
      },
      "relation": {
        "correct": 82,
        "incorrect": 22,
        "accuracy": 78.84615384615384
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "all_true_indices": [
      "221_result.json",
      "166_result.json",
      "106_result.json",
      "253_result.json",
      "261_result.json",
      "285_result.json",
      "123_result.json",
      "189_result.json",
      "162_result.json",
      "149_result.json"
    ],
    "Model": "DeepSeek-R1 (Qwen-14B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "14B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 66,
        "incorrect": 134,
        "accuracy": 33.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 42,
        "incorrect": 54,
        "accuracy": 43.75
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 23,
        "incorrect": 177,
        "accuracy": 11.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 146,
        "incorrect": 54,
        "accuracy": 73.0
      },
      "relation": {
        "correct": 72,
        "incorrect": 32,
        "accuracy": 69.23076923076923
      },
      "bound": {
        "correct": 74,
        "incorrect": 22,
        "accuracy": 77.08333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "103_result.json",
      "106_result.json",
      "261_result.json"
    ],
    "Model": "Gemini 2.0 Flash-Lite",
    "Type": "Chat",
    "Source": "https://deepmind.google/technologies/gemini/flash-lite/",
    "Date": "2025-02-25",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.499999999999998
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 100,
        "incorrect": 4,
        "accuracy": 96.15384615384616
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.54166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Llama-3.1-8B",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "8B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 125,
        "incorrect": 75,
        "accuracy": 62.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.79166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 33,
        "incorrect": 63,
        "accuracy": 34.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 44,
        "incorrect": 156,
        "accuracy": 22.0
      },
      "relation": {
        "correct": 27,
        "incorrect": 77,
        "accuracy": 25.961538461538463
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.708333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 155,
        "incorrect": 45,
        "accuracy": 77.5
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.46153846153846
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 19,
        "incorrect": 181,
        "accuracy": 9.5
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "291_result.json",
      "284_result.json",
      "198_result.json",
      "252_result.json",
      "228_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "166_result.json",
      "106_result.json",
      "289_result.json",
      "190_result.json",
      "253_result.json",
      "243_result.json",
      "135_result.json",
      "162_result.json",
      "149_result.json",
      "296_result.json",
      "229_result.json"
    ],
    "Model": "o3-mini",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3-mini",
    "Date": "2025-01-31",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 25,
        "incorrect": 79,
        "accuracy": 24.03846153846154
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 71,
        "incorrect": 25,
        "accuracy": 73.95833333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 198,
        "incorrect": 2,
        "accuracy": 99.0
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Gemma-2-9B",
    "Type": "Chat",
    "Source": "https://huggingface.co/google/gemma-2-9b-it",
    "Date": "2024-06-25",
    "Max Tokens": "6000",
    "Model Source": "Open-source",
    "Size": "9B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 52,
        "incorrect": 44,
        "accuracy": 54.166666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 28,
        "incorrect": 76,
        "accuracy": 26.923076923076923
      },
      "bound": {
        "correct": 24,
        "incorrect": 72,
        "accuracy": 25.0
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 59,
        "incorrect": 141,
        "accuracy": 29.5
      },
      "relation": {
        "correct": 39,
        "incorrect": 65,
        "accuracy": 37.5
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 79,
        "incorrect": 17,
        "accuracy": 82.29166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "275_result.json",
      "182_result.json",
      "261_result.json",
      "149_result.json"
    ],
    "Model": "QwQ-32B",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/QwQ-32B",
    "Date": "2025-03-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 57,
        "incorrect": 39,
        "accuracy": 59.375
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 31,
        "incorrect": 169,
        "accuracy": 15.5
      },
      "relation": {
        "correct": 18,
        "incorrect": 86,
        "accuracy": 17.307692307692307
      },
      "bound": {
        "correct": 13,
        "incorrect": 83,
        "accuracy": 13.541666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 111,
        "incorrect": 89,
        "accuracy": 55.50000000000001
      },
      "relation": {
        "correct": 53,
        "incorrect": 51,
        "accuracy": 50.96153846153846
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "calc_judge": {
      "total": {
        "correct": 189,
        "incorrect": 11,
        "accuracy": 94.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "198_result.json",
      "103_result.json",
      "106_result.json",
      "261_result.json",
      "243_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.0 Flash",
    "Type": "Chat",
    "Source": "https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash",
    "Date": "2025-02-05",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 22,
        "incorrect": 178,
        "accuracy": 11.0
      },
      "relation": {
        "correct": 15,
        "incorrect": 89,
        "accuracy": 14.423076923076922
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 164,
        "incorrect": 36,
        "accuracy": 82.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 69,
        "incorrect": 27,
        "accuracy": 71.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "approx_judge": {
      "total": {
        "correct": 197,
        "incorrect": 3,
        "accuracy": 98.5
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 0,
        "incorrect": 200,
        "accuracy": 0.0
      },
      "relation": {
        "correct": 0,
        "incorrect": 104,
        "accuracy": 0.0
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [],
    "Model": "Llama-3.2-3B",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct",
    "Date": "2024-09-25",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "3B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 82,
        "incorrect": 118,
        "accuracy": 41.0
      },
      "relation": {
        "correct": 48,
        "incorrect": 56,
        "accuracy": 46.15384615384615
      },
      "bound": {
        "correct": 34,
        "incorrect": 62,
        "accuracy": 35.41666666666667
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 41,
        "incorrect": 63,
        "accuracy": 39.42307692307692
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.208333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "approx_judge": {
      "total": {
        "correct": 62,
        "incorrect": 138,
        "accuracy": 31.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "291_result.json",
      "284_result.json",
      "275_result.json",
      "253_result.json",
      "261_result.json",
      "123_result.json",
      "189_result.json",
      "243_result.json",
      "293_result.json",
      "149_result.json",
      "262_result.json",
      "229_result.json"
    ],
    "Model": "Qwen3-235B-A22B",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/Qwen3-235B-A22B-FP8",
    "Date": "2025-04-28",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "235B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 107,
        "incorrect": 93,
        "accuracy": 53.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.03846153846153
      },
      "bound": {
        "correct": 56,
        "incorrect": 40,
        "accuracy": 58.333333333333336
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 46,
        "incorrect": 154,
        "accuracy": 23.0
      },
      "relation": {
        "correct": 23,
        "incorrect": 81,
        "accuracy": 22.115384615384613
      },
      "bound": {
        "correct": 23,
        "incorrect": 73,
        "accuracy": 23.958333333333336
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 52,
        "incorrect": 148,
        "accuracy": 26.0
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 20,
        "incorrect": 76,
        "accuracy": 20.833333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 71,
        "incorrect": 129,
        "accuracy": 35.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 37,
        "incorrect": 59,
        "accuracy": 38.54166666666667
      }
    },
    "calc_judge": {
      "total": {
        "correct": 174,
        "incorrect": 26,
        "accuracy": 87.0
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "all_true_indices": [
      "184_result.json",
      "221_result.json",
      "124_result.json",
      "101_result.json",
      "261_result.json",
      "162_result.json",
      "149_result.json"
    ],
    "Model": "DeepSeek-R1 (Llama-70B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Llama-70B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "70B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 51,
        "incorrect": 53,
        "accuracy": 49.03846153846153
      },
      "bound": {
        "correct": 30,
        "incorrect": 66,
        "accuracy": 31.25
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 32,
        "incorrect": 168,
        "accuracy": 16.0
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 20,
        "incorrect": 180,
        "accuracy": 10.0
      },
      "relation": {
        "correct": 12,
        "incorrect": 92,
        "accuracy": 11.538461538461538
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.333333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 119,
        "incorrect": 81,
        "accuracy": 59.5
      },
      "relation": {
        "correct": 55,
        "incorrect": 49,
        "accuracy": 52.88461538461539
      },
      "bound": {
        "correct": 64,
        "incorrect": 32,
        "accuracy": 66.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [
      "252_result.json",
      "289_result.json",
      "218_result.json",
      "261_result.json",
      "293_result.json"
    ],
    "Model": "GPT-4.1",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4.1",
    "Date": "2025-04-14",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 29,
        "incorrect": 171,
        "accuracy": 14.499999999999998
      },
      "relation": {
        "correct": 13,
        "incorrect": 91,
        "accuracy": 12.5
      },
      "bound": {
        "correct": 16,
        "incorrect": 80,
        "accuracy": 16.666666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 40,
        "incorrect": 160,
        "accuracy": 20.0
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.884615384615387
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.730769230769231
      },
      "bound": {
        "correct": 5,
        "incorrect": 91,
        "accuracy": 5.208333333333334
      }
    },
    "approx_judge": {
      "total": {
        "correct": 96,
        "incorrect": 104,
        "accuracy": 48.0
      },
      "relation": {
        "correct": 49,
        "incorrect": 55,
        "accuracy": 47.11538461538461
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 48.95833333333333
      }
    },
    "calc_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 83,
        "incorrect": 21,
        "accuracy": 79.8076923076923
      },
      "bound": {
        "correct": 84,
        "incorrect": 12,
        "accuracy": 87.5
      }
    },
    "all_true": {
      "total": {
        "correct": 1,
        "incorrect": 199,
        "accuracy": 0.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 0,
        "incorrect": 96,
        "accuracy": 0.0
      }
    },
    "all_true_indices": [
      "282_result.json"
    ],
    "Model": "DeepSeek-R1 (Qwen-14B)",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
    "Date": "2025-01-20",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "1.5B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 42,
        "incorrect": 62,
        "accuracy": 40.38461538461539
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 56,
        "incorrect": 144,
        "accuracy": 28.000000000000004
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.807692307692307
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 60,
        "incorrect": 140,
        "accuracy": 30.0
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "approx_judge": {
      "total": {
        "correct": 45,
        "incorrect": 155,
        "accuracy": 22.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 25,
        "incorrect": 71,
        "accuracy": 26.041666666666668
      }
    },
    "calc_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 94,
        "incorrect": 10,
        "accuracy": 90.38461538461539
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "284_result.json",
      "252_result.json",
      "106_result.json",
      "149_result.json"
    ],
    "Model": "QwQ-32B-preview",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/Qwen/QwQ-32B-Preview",
    "Date": "2024-11-27",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 32,
        "incorrect": 72,
        "accuracy": 30.76923076923077
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 72,
        "incorrect": 128,
        "accuracy": 36.0
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 26,
        "incorrect": 70,
        "accuracy": 27.083333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 181,
        "incorrect": 19,
        "accuracy": 90.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 86,
        "incorrect": 10,
        "accuracy": 89.58333333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "210_result.json",
      "192_result.json",
      "253_result.json"
    ],
    "Model": "Qwen2.5-Coder-32B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
    "Date": "2024-11-10",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "32B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 70,
        "incorrect": 130,
        "accuracy": 35.0
      },
      "relation": {
        "correct": 31,
        "incorrect": 73,
        "accuracy": 29.807692307692307
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 89,
        "incorrect": 111,
        "accuracy": 44.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 31,
        "incorrect": 65,
        "accuracy": 32.29166666666667
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "192_result.json",
      "203_result.json",
      "106_result.json",
      "253_result.json",
      "261_result.json",
      "229_result.json"
    ],
    "Model": "Qwen2.5-7B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "7B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 64,
        "incorrect": 40,
        "accuracy": 61.53846153846154
      },
      "bound": {
        "correct": 72,
        "incorrect": 24,
        "accuracy": 75.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 175,
        "incorrect": 25,
        "accuracy": 87.5
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 85,
        "incorrect": 11,
        "accuracy": 88.54166666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 126,
        "incorrect": 74,
        "accuracy": 63.0
      },
      "relation": {
        "correct": 68,
        "incorrect": 36,
        "accuracy": 65.38461538461539
      },
      "bound": {
        "correct": 58,
        "incorrect": 38,
        "accuracy": 60.416666666666664
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 196,
        "incorrect": 4,
        "accuracy": 98.0
      },
      "relation": {
        "correct": 102,
        "incorrect": 2,
        "accuracy": 98.07692307692307
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 87,
        "incorrect": 113,
        "accuracy": 43.5
      },
      "relation": {
        "correct": 40,
        "incorrect": 64,
        "accuracy": 38.46153846153847
      },
      "bound": {
        "correct": 47,
        "incorrect": 49,
        "accuracy": 48.95833333333333
      }
    },
    "all_true_indices": [
      "291_result.json",
      "119_result.json",
      "188_result.json",
      "270_result.json",
      "200_result.json",
      "298_result.json",
      "110_result.json",
      "227_result.json",
      "257_result.json",
      "122_result.json",
      "284_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "157_result.json",
      "109_result.json",
      "142_result.json",
      "132_result.json",
      "205_result.json",
      "288_result.json",
      "275_result.json",
      "100_result.json",
      "170_result.json",
      "103_result.json",
      "258_result.json",
      "206_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "124_result.json",
      "138_result.json",
      "116_result.json",
      "182_result.json",
      "163_result.json",
      "113_result.json",
      "266_result.json",
      "158_result.json",
      "128_result.json",
      "203_result.json",
      "176_result.json",
      "106_result.json",
      "231_result.json",
      "197_result.json",
      "134_result.json",
      "144_result.json",
      "289_result.json",
      "171_result.json",
      "101_result.json",
      "190_result.json",
      "133_result.json",
      "218_result.json",
      "268_result.json",
      "280_result.json",
      "108_result.json",
      "253_result.json",
      "164_result.json",
      "114_result.json",
      "261_result.json",
      "211_result.json",
      "256_result.json",
      "180_result.json",
      "278_result.json",
      "208_result.json",
      "111_result.json",
      "104_result.json",
      "189_result.json",
      "201_result.json",
      "168_result.json",
      "243_result.json",
      "145_result.json",
      "135_result.json",
      "159_result.json",
      "107_result.json",
      "162_result.json",
      "286_result.json",
      "120_result.json",
      "225_result.json",
      "149_result.json",
      "262_result.json",
      "139_result.json",
      "212_result.json",
      "130_result.json",
      "235_result.json",
      "193_result.json",
      "172_result.json",
      "229_result.json"
    ],
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "30000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 136,
        "incorrect": 64,
        "accuracy": 68.0
      },
      "relation": {
        "correct": 69,
        "incorrect": 35,
        "accuracy": 66.34615384615384
      },
      "bound": {
        "correct": 67,
        "incorrect": 29,
        "accuracy": 69.79166666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 57,
        "incorrect": 143,
        "accuracy": 28.499999999999996
      },
      "relation": {
        "correct": 30,
        "incorrect": 74,
        "accuracy": 28.846153846153843
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 21,
        "incorrect": 83,
        "accuracy": 20.192307692307693
      },
      "bound": {
        "correct": 17,
        "incorrect": 79,
        "accuracy": 17.708333333333336
      }
    },
    "approx_judge": {
      "total": {
        "correct": 167,
        "incorrect": 33,
        "accuracy": 83.5
      },
      "relation": {
        "correct": 77,
        "incorrect": 27,
        "accuracy": 74.03846153846155
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 191,
        "incorrect": 9,
        "accuracy": 95.5
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 15,
        "incorrect": 185,
        "accuracy": 7.5
      },
      "relation": {
        "correct": 9,
        "incorrect": 95,
        "accuracy": 8.653846153846153
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "all_true_indices": [
      "192_result.json",
      "221_result.json",
      "282_result.json",
      "266_result.json",
      "106_result.json",
      "289_result.json",
      "171_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "293_result.json",
      "162_result.json",
      "149_result.json",
      "235_result.json",
      "229_result.json"
    ],
    "Model": "o1",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o1",
    "Date": "2024-12-17",
    "Max Tokens": "40000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 84,
        "incorrect": 116,
        "accuracy": 42.0
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 49,
        "incorrect": 47,
        "accuracy": 51.041666666666664
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 109,
        "incorrect": 91,
        "accuracy": 54.50000000000001
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 182,
        "incorrect": 18,
        "accuracy": 91.0
      },
      "relation": {
        "correct": 91,
        "incorrect": 13,
        "accuracy": 87.5
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 2,
        "incorrect": 102,
        "accuracy": 1.9230769230769231
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "103_result.json",
      "154_result.json",
      "163_result.json",
      "253_result.json",
      "293_result.json"
    ],
    "Model": "Qwen2.5-72B",
    "Type": "Chat",
    "Source": "https://huggingface.co/Qwen/Qwen2.5-72B-Instruct",
    "Date": "2024-09-16",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "72B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 83,
        "incorrect": 117,
        "accuracy": 41.5
      },
      "relation": {
        "correct": 35,
        "incorrect": 69,
        "accuracy": 33.65384615384615
      },
      "bound": {
        "correct": 48,
        "incorrect": 48,
        "accuracy": 50.0
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 98,
        "incorrect": 102,
        "accuracy": 49.0
      },
      "relation": {
        "correct": 66,
        "incorrect": 38,
        "accuracy": 63.46153846153846
      },
      "bound": {
        "correct": 32,
        "incorrect": 64,
        "accuracy": 33.33333333333333
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 93,
        "incorrect": 11,
        "accuracy": 89.42307692307693
      },
      "bound": {
        "correct": 94,
        "incorrect": 2,
        "accuracy": 97.91666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 184,
        "incorrect": 16,
        "accuracy": 92.0
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "all_true": {
      "total": {
        "correct": 2,
        "incorrect": 198,
        "accuracy": 1.0
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "258_result.json",
      "106_result.json"
    ],
    "Model": "Claude 3.7 Sonnet",
    "Type": "Reasoning",
    "Source": "https://www.anthropic.com/news/claude-3-7-sonnet",
    "Date": "2025-02-19",
    "Max Tokens": "8000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 74,
        "incorrect": 126,
        "accuracy": 37.0
      },
      "relation": {
        "correct": 45,
        "incorrect": 59,
        "accuracy": 43.269230769230774
      },
      "bound": {
        "correct": 29,
        "incorrect": 67,
        "accuracy": 30.208333333333332
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 187,
        "incorrect": 13,
        "accuracy": 93.5
      },
      "relation": {
        "correct": 99,
        "incorrect": 5,
        "accuracy": 95.1923076923077
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 52,
        "incorrect": 52,
        "accuracy": 50.0
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 183,
        "incorrect": 17,
        "accuracy": 91.5
      },
      "relation": {
        "correct": 88,
        "incorrect": 16,
        "accuracy": 84.61538461538461
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 194,
        "incorrect": 6,
        "accuracy": 97.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 93,
        "incorrect": 3,
        "accuracy": 96.875
      }
    },
    "all_true": {
      "total": {
        "correct": 42,
        "incorrect": 158,
        "accuracy": 21.0
      },
      "relation": {
        "correct": 24,
        "incorrect": 80,
        "accuracy": 23.076923076923077
      },
      "bound": {
        "correct": 18,
        "incorrect": 78,
        "accuracy": 18.75
      }
    },
    "all_true_indices": [
      "291_result.json",
      "232_result.json",
      "194_result.json",
      "298_result.json",
      "152_result.json",
      "198_result.json",
      "210_result.json",
      "260_result.json",
      "184_result.json",
      "237_result.json",
      "142_result.json",
      "132_result.json",
      "275_result.json",
      "103_result.json",
      "258_result.json",
      "192_result.json",
      "234_result.json",
      "221_result.json",
      "282_result.json",
      "166_result.json",
      "176_result.json",
      "106_result.json",
      "197_result.json",
      "289_result.json",
      "101_result.json",
      "190_result.json",
      "218_result.json",
      "108_result.json",
      "253_result.json",
      "199_result.json",
      "114_result.json",
      "261_result.json",
      "243_result.json",
      "196_result.json",
      "293_result.json",
      "135_result.json",
      "107_result.json",
      "162_result.json",
      "149_result.json",
      "262_result.json",
      "212_result.json",
      "229_result.json"
    ],
    "Model": "o3",
    "Type": "Reasoning",
    "Source": "https://platform.openai.com/docs/models/o3",
    "Date": "2025-04-16",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Reasoning effort": "medium",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 75,
        "incorrect": 125,
        "accuracy": 37.5
      },
      "relation": {
        "correct": 36,
        "incorrect": 68,
        "accuracy": 34.61538461538461
      },
      "bound": {
        "correct": 39,
        "incorrect": 57,
        "accuracy": 40.625
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 64,
        "incorrect": 136,
        "accuracy": 32.0
      },
      "relation": {
        "correct": 43,
        "incorrect": 61,
        "accuracy": 41.34615384615385
      },
      "bound": {
        "correct": 21,
        "incorrect": 75,
        "accuracy": 21.875
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "approx_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "calc_judge": {
      "total": {
        "correct": 188,
        "incorrect": 12,
        "accuracy": 94.0
      },
      "relation": {
        "correct": 98,
        "incorrect": 6,
        "accuracy": 94.23076923076923
      },
      "bound": {
        "correct": 90,
        "incorrect": 6,
        "accuracy": 93.75
      }
    },
    "all_true": {
      "total": {
        "correct": 6,
        "incorrect": 194,
        "accuracy": 3.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "165_result.json",
      "210_result.json",
      "294_result.json",
      "192_result.json",
      "199_result.json",
      "261_result.json"
    ],
    "Model": "GPT-4o",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4o",
    "Date": "2024-08-06",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 67,
        "incorrect": 133,
        "accuracy": 33.5
      },
      "relation": {
        "correct": 22,
        "incorrect": 82,
        "accuracy": 21.153846153846153
      },
      "bound": {
        "correct": 45,
        "incorrect": 51,
        "accuracy": 46.875
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 61,
        "incorrect": 139,
        "accuracy": 30.5
      },
      "relation": {
        "correct": 46,
        "incorrect": 58,
        "accuracy": 44.230769230769226
      },
      "bound": {
        "correct": 15,
        "incorrect": 81,
        "accuracy": 15.625
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 7,
        "incorrect": 193,
        "accuracy": 3.5000000000000004
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 91,
        "incorrect": 5,
        "accuracy": 94.79166666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 185,
        "incorrect": 15,
        "accuracy": 92.5
      },
      "relation": {
        "correct": 96,
        "incorrect": 8,
        "accuracy": 92.3076923076923
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 3,
        "incorrect": 197,
        "accuracy": 1.5
      },
      "relation": {
        "correct": 1,
        "incorrect": 103,
        "accuracy": 0.9615384615384616
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "228_result.json",
      "192_result.json",
      "106_result.json"
    ],
    "Model": "Llama-4-Scout",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "16 x 17B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 81,
        "incorrect": 119,
        "accuracy": 40.5
      },
      "relation": {
        "correct": 37,
        "incorrect": 67,
        "accuracy": 35.57692307692308
      },
      "bound": {
        "correct": 44,
        "incorrect": 52,
        "accuracy": 45.83333333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 85,
        "incorrect": 115,
        "accuracy": 42.5
      },
      "relation": {
        "correct": 58,
        "incorrect": 46,
        "accuracy": 55.769230769230774
      },
      "bound": {
        "correct": 27,
        "incorrect": 69,
        "accuracy": 28.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 8,
        "incorrect": 192,
        "accuracy": 4.0
      },
      "relation": {
        "correct": 4,
        "incorrect": 100,
        "accuracy": 3.8461538461538463
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "approx_judge": {
      "total": {
        "correct": 178,
        "incorrect": 22,
        "accuracy": 89.0
      },
      "relation": {
        "correct": 90,
        "incorrect": 14,
        "accuracy": 86.53846153846155
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 101,
        "incorrect": 3,
        "accuracy": 97.11538461538461
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "all_true_indices": [
      "198_result.json",
      "103_result.json",
      "258_result.json",
      "106_result.json",
      "274_result.json"
    ],
    "Model": "Llama-4-Maverick",
    "Type": "Chat",
    "Source": "https://huggingface.co/meta-llama/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "Date": "2025-04-05",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "128 x 17B",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 79,
        "incorrect": 121,
        "accuracy": 39.5
      },
      "relation": {
        "correct": 38,
        "incorrect": 66,
        "accuracy": 36.53846153846153
      },
      "bound": {
        "correct": 41,
        "incorrect": 55,
        "accuracy": 42.70833333333333
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 58,
        "incorrect": 142,
        "accuracy": 28.999999999999996
      },
      "relation": {
        "correct": 47,
        "incorrect": 57,
        "accuracy": 45.19230769230769
      },
      "bound": {
        "correct": 11,
        "incorrect": 85,
        "accuracy": 11.458333333333332
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 5,
        "incorrect": 195,
        "accuracy": 2.5
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 2,
        "incorrect": 94,
        "accuracy": 2.083333333333333
      }
    },
    "approx_judge": {
      "total": {
        "correct": 180,
        "incorrect": 20,
        "accuracy": 90.0
      },
      "relation": {
        "correct": 92,
        "incorrect": 12,
        "accuracy": 88.46153846153845
      },
      "bound": {
        "correct": 88,
        "incorrect": 8,
        "accuracy": 91.66666666666666
      }
    },
    "calc_judge": {
      "total": {
        "correct": 186,
        "incorrect": 14,
        "accuracy": 93.0
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 89,
        "incorrect": 7,
        "accuracy": 92.70833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 4,
        "incorrect": 196,
        "accuracy": 2.0
      },
      "relation": {
        "correct": 3,
        "incorrect": 101,
        "accuracy": 2.8846153846153846
      },
      "bound": {
        "correct": 1,
        "incorrect": 95,
        "accuracy": 1.0416666666666665
      }
    },
    "all_true_indices": [
      "210_result.json",
      "192_result.json",
      "253_result.json",
      "199_result.json"
    ],
    "Model": "GPT-4o mini",
    "Type": "Chat",
    "Source": "https://platform.openai.com/docs/models/gpt-4o-mini",
    "Date": "2024-07-18",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 14,
        "incorrect": 186,
        "accuracy": 7.000000000000001
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 8,
        "incorrect": 88,
        "accuracy": 8.333333333333332
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 177,
        "incorrect": 23,
        "accuracy": 88.5
      },
      "relation": {
        "correct": 97,
        "incorrect": 7,
        "accuracy": 93.26923076923077
      },
      "bound": {
        "correct": 80,
        "incorrect": 16,
        "accuracy": 83.33333333333334
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 38,
        "incorrect": 162,
        "accuracy": 19.0
      },
      "relation": {
        "correct": 26,
        "incorrect": 78,
        "accuracy": 25.0
      },
      "bound": {
        "correct": 12,
        "incorrect": 84,
        "accuracy": 12.5
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 199,
        "incorrect": 1,
        "accuracy": 99.5
      },
      "relation": {
        "correct": 103,
        "incorrect": 1,
        "accuracy": 99.03846153846155
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 12,
        "incorrect": 188,
        "accuracy": 6.0
      },
      "relation": {
        "correct": 5,
        "incorrect": 99,
        "accuracy": 4.807692307692308
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "all_true_indices": [
      "103_result.json",
      "234_result.json",
      "221_result.json",
      "116_result.json",
      "163_result.json",
      "128_result.json",
      "106_result.json",
      "231_result.json",
      "289_result.json",
      "261_result.json",
      "135_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.5 Pro",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-pro",
    "Date": "2025-03-25",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 11,
        "incorrect": 189,
        "accuracy": 5.5
      },
      "relation": {
        "correct": 7,
        "incorrect": 97,
        "accuracy": 6.730769230769231
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 176,
        "incorrect": 24,
        "accuracy": 88.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 81,
        "incorrect": 15,
        "accuracy": 84.375
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 27,
        "incorrect": 173,
        "accuracy": 13.5
      },
      "relation": {
        "correct": 20,
        "incorrect": 84,
        "accuracy": 19.230769230769234
      },
      "bound": {
        "correct": 7,
        "incorrect": 89,
        "accuracy": 7.291666666666667
      }
    },
    "approx_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "calc_judge": {
      "total": {
        "correct": 200,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "relation": {
        "correct": 104,
        "incorrect": 0,
        "accuracy": 100.0
      },
      "bound": {
        "correct": 96,
        "incorrect": 0,
        "accuracy": 100.0
      }
    },
    "all_true": {
      "total": {
        "correct": 9,
        "incorrect": 191,
        "accuracy": 4.5
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 3,
        "incorrect": 93,
        "accuracy": 3.125
      }
    },
    "all_true_indices": [
      "198_result.json",
      "205_result.json",
      "103_result.json",
      "234_result.json",
      "106_result.json",
      "218_result.json",
      "253_result.json",
      "261_result.json",
      "149_result.json"
    ],
    "Model": "Gemini 2.5 Flash",
    "Type": "Reasoning",
    "Source": "https://cloud.google.com/vertex-ai/generative-ai/docs/models/gemini/2-5-flash",
    "Date": "2025-04-17",
    "Max Tokens": "10000",
    "Model Source": "Proprietary",
    "Size": "",
    "Email": "",
    "timestamp": ""
  },
  {
    "is_correct": {
      "total": {
        "correct": 99,
        "incorrect": 101,
        "accuracy": 49.5
      },
      "relation": {
        "correct": 34,
        "incorrect": 70,
        "accuracy": 32.69230769230769
      },
      "bound": {
        "correct": 65,
        "incorrect": 31,
        "accuracy": 67.70833333333334
      }
    },
    "toy_case_judge": {
      "total": {
        "correct": 114,
        "incorrect": 86,
        "accuracy": 56.99999999999999
      },
      "relation": {
        "correct": 63,
        "incorrect": 41,
        "accuracy": 60.57692307692307
      },
      "bound": {
        "correct": 51,
        "incorrect": 45,
        "accuracy": 53.125
      }
    },
    "logical_gap_judge": {
      "total": {
        "correct": 35,
        "incorrect": 165,
        "accuracy": 17.5
      },
      "relation": {
        "correct": 29,
        "incorrect": 75,
        "accuracy": 27.884615384615387
      },
      "bound": {
        "correct": 6,
        "incorrect": 90,
        "accuracy": 6.25
      }
    },
    "approx_judge": {
      "total": {
        "correct": 162,
        "incorrect": 38,
        "accuracy": 81.0
      },
      "relation": {
        "correct": 70,
        "incorrect": 34,
        "accuracy": 67.3076923076923
      },
      "bound": {
        "correct": 92,
        "incorrect": 4,
        "accuracy": 95.83333333333334
      }
    },
    "calc_judge": {
      "total": {
        "correct": 190,
        "incorrect": 10,
        "accuracy": 95.0
      },
      "relation": {
        "correct": 95,
        "incorrect": 9,
        "accuracy": 91.34615384615384
      },
      "bound": {
        "correct": 95,
        "incorrect": 1,
        "accuracy": 98.95833333333334
      }
    },
    "all_true": {
      "total": {
        "correct": 10,
        "incorrect": 190,
        "accuracy": 5.0
      },
      "relation": {
        "correct": 6,
        "incorrect": 98,
        "accuracy": 5.769230769230769
      },
      "bound": {
        "correct": 4,
        "incorrect": 92,
        "accuracy": 4.166666666666666
      }
    },
    "all_true_indices": [
      "270_result.json",
      "284_result.json",
      "192_result.json",
      "266_result.json",
      "106_result.json",
      "218_result.json",
      "253_result.json",
      "114_result.json",
      "123_result.json",
      "243_result.json"
    ],
    "Model": "DeepSeek-R1",
    "Type": "Reasoning",
    "Source": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
    "Date": "2025-01-19",
    "Max Tokens": "10000",
    "Model Source": "Open-source",
    "Size": "",
    "Email": "",
    "timestamp": ""
  }
]